{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBd8DWz18WHC"
      },
      "outputs": [],
      "source": [
        "# Perform Google Colab installs (if running in Google Colab)\n",
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "    !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!pip install webdriver_manager\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver"
      ],
      "metadata": {
        "id": "z6GBYCdXqSRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "blcz_NgX5mWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from bs4.element import Tag\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "def getglinks(query):\n",
        "    options = Options()\n",
        "    options.add_argument(\"--start-maximized\")\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--start-maximized\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    options.add_experimental_option('useAutomationExtension', False)\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    google_url = \"https://www.google.com/search?q=\"\n",
        "    google_url+=\"+\".join(query.split())\n",
        "    driver.get(google_url)\n",
        "    time.sleep(3)\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
        "    result_div = soup.find_all('div', attrs={'class': 'g'})\n",
        "\n",
        "\n",
        "    links = []\n",
        "    titles = []\n",
        "    descriptions = []\n",
        "    for r in result_div:\n",
        "        # Checks if each element is present, else, raise exception\n",
        "        try:\n",
        "            link = r.find('a', href=True)\n",
        "            title = None\n",
        "            title = r.find('h3')\n",
        "\n",
        "            if isinstance(title,Tag):\n",
        "                title = title.get_text()\n",
        "\n",
        "            description = None\n",
        "            description = r.find('span', attrs={'class': 'st'})\n",
        "\n",
        "            if isinstance(description, Tag):\n",
        "                description = description.get_text()\n",
        "\n",
        "            # Check to make sure everything is present before appending\n",
        "            if link != '' and title != '' and description != '':\n",
        "                links.append(link['href'])\n",
        "                titles.append(title)\n",
        "                descriptions.append(description)\n",
        "        # Next loop if one element is not present\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            continue\n",
        "    links=[i for i in links if(i[:4])==\"http\"]\n",
        "    return links\n"
      ],
      "metadata": {
        "id": "2wll4qLdqLaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change Prompt Here"
      ],
      "metadata": {
        "id": "-2lr0tKHIRmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Who is the current president of India?\""
      ],
      "metadata": {
        "id": "i7EwgCPdrXwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "def getresults(query):\n",
        "    links=getglinks(query)\n",
        "    f4=links[:40]\n",
        "    blacklist = [\n",
        "        '[document]',\n",
        "\n",
        "    'noscript',\n",
        "    'script',\n",
        "        'header',\n",
        "        'style',\n",
        "        'html',\n",
        "        'meta',\n",
        "        'head',\n",
        "        'input',\n",
        "        'script',\n",
        "        'a',\n",
        "    ]\n",
        "    output=[]\n",
        "    for url in f4:\n",
        "      print(url)\n",
        "      html=requests.get(url,verify=False).text\n",
        "      soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "      #soup = BeautifulSoup(r.co, 'html5lib') # If this line causes an error, run 'pip install html5lib' or install html5lib\n",
        "      # kill all script and style elements\n",
        "      for script in soup([\"noscript\",\"script\", \"style\",\"meta\",\"a\",\"[document]\"]):\n",
        "          script.extract()    # rip it out\n",
        "\n",
        "      text = soup.get_text()\n",
        "      lines = (line.strip() for line in text.splitlines())\n",
        "      chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "      for t in chunks:\n",
        "          output.append('{} '.format(t))\n",
        "    return output\n",
        "RetrievedfromGoogle=getresults(prompt)"
      ],
      "metadata": {
        "id": "aIa1TtxfiUPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(RetrievedfromGoogle)"
      ],
      "metadata": {
        "id": "sLYSTpxS8D_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RetrievedfromGoogle=[i for i in RetrievedfromGoogle if len(i)>4]"
      ],
      "metadata": {
        "id": "qOa0mVb7vWWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for i in RetrievedfromGoogle:\n",
        "#  if(\"Murmu\" in i):\n",
        "#    print(\"Yes\")"
      ],
      "metadata": {
        "id": "axRgoLxnvWUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_formatter(textList):\n",
        "    \"\"\"Performs minor formatting on text.\"\"\"\n",
        "    cleaned_textList=[]\n",
        "    for text in textList:\n",
        "      cleaned_text=text.replace(\"\\n\", \".\").strip()\n",
        "      cleaned_text=\" \".join(filter(lambda x:x[:3]!='www', cleaned_text.split()))\n",
        "      cleaned_text = cleaned_text.replace(\"ï¿½\", \" \").strip()\n",
        "      if(len(cleaned_text)>1):\n",
        "        cleaned_textList.append(cleaned_text)\n",
        "      # Other potential text formatting functions can go here\n",
        "    return cleaned_textList\n",
        "RetrievedfromGoogle=text_formatter(RetrievedfromGoogle)\n"
      ],
      "metadata": {
        "id": "Ji1QxqVVh4sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RetrievedfromGoogle=\" \".join(RetrievedfromGoogle)\n",
        "type(RetrievedfromGoogle)"
      ],
      "metadata": {
        "id": "Oh4IWm2zq3jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(RetrievedfromGoogle)"
      ],
      "metadata": {
        "id": "2DUYJSXtvDYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2qThVs78WHF"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "nlp.add_pipe(\"sentencizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po3el1dZ8WHG"
      },
      "outputs": [],
      "source": [
        "RetrievedfromGoogle = list(nlp(RetrievedfromGoogle).sents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(RetrievedfromGoogle)):\n",
        "  RetrievedfromGoogle[i]=str(RetrievedfromGoogle[i])"
      ],
      "metadata": {
        "id": "Y6OZa3D7rIzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "A1FA5n6BmQQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall torch -y\n",
        "#!pip uninstall torchvision -y"
      ],
      "metadata": {
        "id": "kqyWtmLHTOWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch\n",
        "#!pip install torchvision"
      ],
      "metadata": {
        "id": "i-PP80d7TaJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDK6PPJ08WHL"
      },
      "outputs": [],
      "source": [
        "# Requires !pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                                      device=\"cuda\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VhUHZP4DTNSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "d0BgkiLNsl_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item=dict()"
      ],
      "metadata": {
        "id": "ffqTmiJ0tJUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_k48dqS8WHM"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Send the model to the GPU\n",
        "embedding_model.to(\"cuda\") # requires a GPU installed, for reference on my local machine, I'm using a NVIDIA RTX 4090\n",
        "item[\"embedding\"]=[]\n",
        "item[\"sentences\"]=[]\n",
        "# Create embeddings one by one on the GPU\n",
        "for sent in tqdm.tqdm(RetrievedfromGoogle):\n",
        "    item[\"sentences\"].append(sent)\n",
        "    item[\"embedding\"].append(embedding_model.encode(sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngPHVgJG8WHO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "\n",
        "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
        "embeddings = torch.tensor(np.array(item[\"embedding\"]), dtype=torch.float32).to(device)\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import util"
      ],
      "metadata": {
        "id": "GJX47o7uUuGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgbwdA_f8WHP"
      },
      "outputs": [],
      "source": [
        "query = prompt\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "from time import perf_counter as timer\n",
        "\n",
        "start_time = timer()\n",
        "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
        "end_time = timer()\n",
        "\n",
        "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
        "\n",
        "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
        "top_results_dot_product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTT837_68WHP"
      },
      "outputs": [],
      "source": [
        "# Define helper function to print wrapped text\n",
        "import textwrap\n",
        "\n",
        "def print_wrapped(text, wrap_length=80):\n",
        "    wrapped_text = textwrap.fill(text, wrap_length)\n",
        "    print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "item[\"sentences\"]"
      ],
      "metadata": {
        "id": "2cTYfgsEDmk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22yMu5Zy8WHQ"
      },
      "outputs": [],
      "source": [
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"Results:\")\n",
        "# Loop through zipped together scores and indicies from torch.topk\n",
        "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
        "    print(\"Text:\")\n",
        "    print_wrapped(item[\"sentences\"][idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etQ4E_FB8WHS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def dot_product(vector1, vector2):\n",
        "    return torch.dot(vector1, vector2)\n",
        "\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    dot_product = torch.dot(vector1, vector2)\n",
        "\n",
        "    # Get Euclidean/L2 norm of each vector (removes the magnitude, keeps direction)\n",
        "    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n",
        "    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n",
        "\n",
        "    return dot_product / (norm_vector1 * norm_vector2)\n",
        "\n",
        "# Example tensors\n",
        "vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
        "vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)\n",
        "\n",
        "# Calculate dot product\n",
        "print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n",
        "print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n",
        "print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n",
        "\n",
        "# Calculate cosine similarity\n",
        "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n",
        "print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n",
        "print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbFIwS3y8WHS"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_resources(query: str,\n",
        "                                embeddings: torch.tensor,\n",
        "                                model: SentenceTransformer=embedding_model,\n",
        "                                n_resources_to_return: int=20,\n",
        "                                print_time: bool=True):\n",
        "    \"\"\"\n",
        "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Embed the query\n",
        "    query_embedding = model.encode(query,\n",
        "                                   convert_to_tensor=True)\n",
        "\n",
        "    # Get dot product scores on embeddings\n",
        "    start_time = timer()\n",
        "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
        "    end_time = timer()\n",
        "\n",
        "    if print_time:\n",
        "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
        "\n",
        "    scores, indices = torch.topk(input=dot_scores,\n",
        "                                 k=n_resources_to_return)\n",
        "\n",
        "    return scores, indices\n",
        "\n",
        "def print_top_results_and_scores(query: str,\n",
        "                                 embeddings: torch.tensor,\n",
        "                                 sentences=item[\"sentences\"],\n",
        "                                 n_resources_to_return: int=5):\n",
        "    \"\"\"\n",
        "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
        "\n",
        "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
        "    \"\"\"\n",
        "\n",
        "    scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                                  embeddings=embeddings,\n",
        "                                                  n_resources_to_return=n_resources_to_return)\n",
        "\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    print(\"Results:\")\n",
        "    # Loop through zipped together scores and indicies\n",
        "    for score, index in zip(scores, indices):\n",
        "        print(f\"Score: {score:.4f}\")\n",
        "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
        "        print_wrapped(item[\"sentences\"][index])\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az5VbpyQ8WHS"
      },
      "source": [
        "Excellent! Now let's test our functions out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaDD9yj08WHT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get just the scores and indices of top related results\n",
        "scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                              embeddings=embeddings)\n",
        "scores, indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MW3swIE8WHT"
      },
      "outputs": [],
      "source": [
        "# Print out the texts of the top scores\n",
        "print_top_results_and_scores(query=query,\n",
        "                             embeddings=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFuO60hy8WHT"
      },
      "outputs": [],
      "source": [
        "# Get GPU available memory\n",
        "import torch\n",
        "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
        "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
        "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1eHyFaf8WHU"
      },
      "outputs": [],
      "source": [
        "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
        "if gpu_memory_gb < 5.1:\n",
        "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
        "elif gpu_memory_gb < 8.1:\n",
        "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
        "    use_quantization_config = True\n",
        "    model_id = \"google/gemma-2b-it\"\n",
        "elif gpu_memory_gb < 19.0:\n",
        "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
        "    use_quantization_config = False\n",
        "    model_id = \"google/gemma-2b-it\"\n",
        "elif gpu_memory_gb > 19.0:\n",
        "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
        "    use_quantization_config = False\n",
        "    model_id = \"google/gemma-7b-it\"\n",
        "\n",
        "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
        "print(f\"model_id set to: {model_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "GZvsdP7eVP3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.utils import is_flash_attn_2_available\n",
        "\n",
        "# 1. Create quantization config for smaller model loading (optional)\n",
        "# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n",
        "# For models that require 4-bit quantization (use this if you have low GPU memory available)\n",
        "from transformers import BitsAndBytesConfig\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                         bnb_4bit_compute_dtype=torch.float16)\n",
        "\n",
        "# Bonus: Setup Flash Attention 2 for faster inference, default to \"sdpa\" or \"scaled dot product attention\" if it's not available\n",
        "# Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus\n",
        "# Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention\n",
        "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
        "  attn_implementation = \"flash_attention_2\"\n",
        "else:\n",
        "  attn_implementation = \"sdpa\"\n",
        "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
        "\n",
        "# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n",
        "#model_id = \"google/gemma-7b-it\"\n",
        "model_id = model_id # (we already set this above)\n",
        "print(f\"[INFO] Using model_id: {model_id}\")\n",
        "\n",
        "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
        "\n",
        "# 4. Instantiate the model\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
        "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
        "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
        "                                                 low_cpu_mem_usage=False, # use full memory\n",
        "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
        "\n",
        "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU\n",
        "    llm_model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "yqy2AbSFU-FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9M5JRp38WHU"
      },
      "outputs": [],
      "source": [
        "llm_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUE4_9Ru8WHV"
      },
      "outputs": [],
      "source": [
        "def get_model_num_params(model: torch.nn.Module):\n",
        "    return sum([param.numel() for param in model.parameters()])\n",
        "\n",
        "get_model_num_params(llm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S33AcNA58WHZ"
      },
      "outputs": [],
      "source": [
        "def prompt_formatter(query,\n",
        "                     context_items) -> str:\n",
        "    \"\"\"\n",
        "    Augments query with text-based context from context_items.\n",
        "    \"\"\"\n",
        "    # Join context items into one dotted paragraph\n",
        "    context = \"- \" + \"\\n- \".join([i for i in context_items])\n",
        "\n",
        "    # Create a base prompt with examples to help the model\n",
        "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
        "    # We could also write this in a txt file and import it in if we wanted.\n",
        "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
        "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
        "Don't return the thinking, only return the answer.\n",
        "Make sure your answers are as explanatory as possible.\n",
        "\\nNow use the following context items to answer the user query:\n",
        "{context}\n",
        "\\nRelevant passages: <extract relevant passages from the context here>\n",
        "User query: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Update base prompt with context items and query\n",
        "    base_prompt = base_prompt.format(context=context, query=query)\n",
        "\n",
        "    # Create prompt template for instruction-tuned model\n",
        "    dialogue_template = [\n",
        "        {\"role\": \"user\",\n",
        "        \"content\": base_prompt}\n",
        "    ]\n",
        "\n",
        "    # Apply the chat template\n",
        "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
        "                                          tokenize=False,\n",
        "                                          add_generation_prompt=True)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCTyURrQ8WHZ"
      },
      "outputs": [],
      "source": [
        "query=prompt\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# Get relevant resources\n",
        "scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                              embeddings=embeddings)\n",
        "\n",
        "# Create a list of context items\n",
        "context_items = [item[\"sentences\"][i] for i in indices]\n",
        "\n",
        "# Format prompt with context items\n",
        "prompt = prompt_formatter(query=query,\n",
        "                          context_items=context_items)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0MQxjX18WHZ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate an output of tokens\n",
        "outputs = llm_model.generate(**input_ids,\n",
        "                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
        "                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n",
        "                             max_new_tokens=256) # how many new tokens to generate from prompt\n",
        "\n",
        "# Turn the output tokens into text\n",
        "output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "Output=f\"Query: {query}\\nRAG answer:\\n{output_text.replace(prompt, '')}\"\n",
        "print(Output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"output.txt\",\"w\") as f:\n",
        "  f.write(Output)\n"
      ],
      "metadata": {
        "id": "wH_mfzktHJRH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}